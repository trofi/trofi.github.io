<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>duperemove speedups</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../css/code.css" />
        <link rel="stylesheet" type="text/css" href="../css/skylighting.css" />

        <link rel="alternate" type="application/atom+xml" href="../feed/atom.xml" title="Atom 1.0" />
        <link rel="alternate" type="application/rss+xml" href="../feed/rss.xml" title="RSS 2.0" />
        <link rel="shortcut icon" href="../images/favicon.ico" />
    </head>
    <body>
        <div id="navigation">
            <a href="../">home</a>
            <a href="../blog.html">blog</a>
            <a href="../log.html">log</a>
            <a href="../feed/atom.xml">atom</a>
            <a href="../feed/rss.xml">rss</a>
            <a href="../about.html">about</a>
        </div>

        <div id="content">
            <h1>duperemove speedups</h1>
            
                <div class="info">November 12, 2023</div>
            

            <p><a href="https://github.com/markfasheh/duperemove/"><code>duperemove</code></a> is a great
tool to reduce on-disk file redundancy for file systems that support
data sharing across files. Currently <code>duperemove</code> supports only <code>btrfs</code>
and <code>xfs</code>.</p>
<h2 id="initial-user-experience">initial user experience</h2>
<p>I started using <code>duperemove</code> around version <code>0.9</code> when I started keeping
multiple <code>chroot</code> environments for various Linux distributions. I had
about 15 instances of various systems:</p>
<pre><code>/chroot/gentoo-amd64-multilib/{bin,lib,usr,...}
/chroot/gentoo-amd64-O3/{bin,lib,usr,...}
/chroot/gentoo-amd64-lto/{bin,lib,usr,...}
/chroot/gentoo-i686/{bin,lib,usr,...}
/chroot/debian-sid/{bin,lib,usr,...}
...</code></pre>
<p>Some of the files had identical or mostly identical content across
systems. The typical source of duplicates would be a
<code>/usr/lib/locale/locale-archive</code> file. Today it contains <code>~200MB</code> worth
of locales on systems with complete set of locales. Another source is
huge dumps of translation files in <code>/usr/share/locale</code>.</p>
<p>As I used <code>btrfs</code> I ran <code>duperemove</code> time to time on a <code>/chroot</code>
directory to claw back a bit of storage eaten by these duplicates. I had
a spinning disk at the time and I was amazed by the speed that
<code>duperemove</code> took to do it’s magic.</p>
<h2 id="more-recent-usage-attempt">more recent usage attempt</h2>
<p>A few years later I got an <code>NVMe</code> <code>SSD</code> storage and started using <code>nix</code>
extensively to build a lot of packages. <code>nix</code> can produce quite a bit of
duplication when building package with minimal changes:</p>
<pre><code>$ ls -1d /nix/store/????????????????????????????????-glibc-2.38-27
/nix/store/4jyz743dan9fn8b53cdxl5fyld2hkaby-glibc-2.38-27
/nix/store/as01fdk2w1605lr3lmpqpwa1xan17gbd-glibc-2.38-27
/nix/store/d9bhmzah59gzbm7bkki7i8fm0p4nyiyv-glibc-2.38-27
/nix/store/iwnxvprzvymiigxds0pw53sxg11m4azk-glibc-2.38-27
/nix/store/ldavvsk4f57lmw816ch0c4v82hf6ww8g-glibc-2.38-27
/nix/store/mk28ys1qrw7mb04psrnmr0p56bxw3g54-glibc-2.38-27
/nix/store/pnnqn1gh3jd7imjjyibgf8r0n0zjrxf5-glibc-2.38-27
/nix/store/ps99gh0kq7yar94ap6ya6d2av2rfa8dz-glibc-2.38-27
/nix/store/qn3ggz5sf3hkjs2c797xf7nan3amdxmp-glibc-2.38-27
/nix/store/qpspr9zw4vxq6fq3rc1izqsglk497m67-glibc-2.38-27
/nix/store/wrv286x4aldgbj6gjl15qn8pl233zrsx-glibc-2.38-27
/nix/store/y2yvsr2gb27ixz8mc42ry4q6lpasl0fk-glibc-2.38-27

$ ls -1d /nix/store/????????????????????????????????-firefox-119.0.1
/nix/store/c3c387alqga9b9s0r4n064d6kkan07dy-firefox-119.0.1
/nix/store/ck5k36nn46vbpc534hvncbana2rmdpxj-firefox-119.0.1
/nix/store/qx36d083630w1ksp3n38avsyk52zxf9j-firefox-119.0.1
/nix/store/r4cjmc042q18bi7xg2jmcxqs8nzl4fr9-firefox-119.0.1
/nix/store/s2qiq9xszj4k7z64ri3lrl1hwqa48v3p-firefox-119.0.1
/nix/store/zns8fsz0c7adk7aw1x11kal6235jxxya-firefox-119.0.1</code></pre>
<p>Sometimes two versions of a package build only differ in the embedded
paths of their dependencies: it’s a 32 byte difference between <code>ELF</code>
files. For the example above <code>duperemove</code> quickly spots the similarity:</p>
<pre><code>$ duperemove -b4096 --batchsize=0 -q --dedupe-options=partial -rd /nix/store/????????????????????????????????-glibc-2.38-27

Simple read and compare of file data found 1174 instances of files that might benefit from deduplication.
Comparison of extent info shows a net change in shared extents of: 266229126
Found 133 identical extents.
[########################################]
Search completed with no errors.
Simple read and compare of file data found 114 instances of extents that might benefit from deduplication.
Comparison of extent info shows a net change in shared extents of: 2183168
Total files scanned:  10636</code></pre>
<p>Here <code>duperemove</code> managed to deduplicate <code>70%</code> (<code>253MB</code> out of <code>357MB</code>
considered) in the extent pass comparison. And the extracted extra <code>2MB</code>
of duplicates when considered <code>4KB</code> blocks within different extents.</p>
<p>I decided to try <code>duperemove</code> on the whole of my <code>/nix/store</code> directory.
I ran <code>duperemove-0.11</code> and got failures related to exhausted file
descriptors: <code>duperemove</code> ran with <code>32x</code> parallelism and was able to hit
<code>4096</code> open files. That was easy to fix by
<a href="https://github.com/markfasheh/duperemove/pull/269">raising the file limit</a>.
I think it worked as fast as before.</p>
<p>But a while after I ran <code>duperemove-0.13</code> against <code>/nix/store</code>. 2 hours
later I found that it did not finish and ate <code>100%</code> of the CPU. That was
unexpected.</p>
<p>I was not sure if it was a particular file that was causing trouble or
the sheer load on <code>duperemove</code> that made it degrade so much under the
load.</p>
<p>I attempted to run <code>duperemove</code> in incremental mode and found out that
it rescans all the files on the database on each run effectively making
the incremental mode quadratic. I filed <a href="https://github.com/markfasheh/duperemove/issues/303">a bug</a>
to see if it could be fixed.</p>
<p>Jack implemented incremental mode the same day! I tried it and saw an
improvement. But the result was still too slow to run on the whole of
<code>/nix/store</code> within a day. I could not easily pinpoint the problem of
<code>100%</code> CPU usage on my workloads.</p>
<h2 id="duperemove-complexity-intuition"><code>duperemove</code> complexity intuition</h2>
<p>What are the <code>duperemove</code>’s scaling limits? I had about <code>4 million</code>
files taking <code>300GB</code> of storage in <code>/nix/store</code> on <code>NVMe</code> device.</p>
<p>Quick quiz: How long should it take to dedupe that data you would say? A
minute, an hour, a day?</p>
<p>In theory all it takes to do is to read all the data out, checksum it
and attempt the deduplication on identified candidates. Should be an
IO-bound problem without too many random reads.</p>
<p>Given that <code>duperemove</code> has an optional <code>sqlite</code> database to persist
details about previous runs it even skips data read of the files it
already processed.</p>
<p>If we have a reasonable fast IO storage capable of 1GB/s of sequential
read throughput then it should ideally take about <code>300GB / 1GB/s = ~5 minutes</code>. And on top of that there should be some minor overhead to
calculate checksums and store some state in <code>sqlite</code> database. That was
my naive reasoning :)</p>
<p>Practice showed that <code>duperemove</code> found the CPU-heavy work to do for
hours on my machine.</p>
<h2 id="synthetic-tests">synthetic tests</h2>
<p>I ran <code>perf top</code> and noticed that <code>duperemove</code> showed unusual reading on
various stages of a run: at one point most of the time was spent in
<code>sqlite</code> internals, at another one some <code>rb_next()</code> function took most
of the time. I did not expect such things in an IO-mostly workload.</p>
<p>As it was not very convenient to experiment with <code>duperemove</code>’s
behaviour on real data I tried to throw synthetic workloads at it.</p>
<p>I started simple: created <code>100 thousands</code> files of <code>1KB</code> size and ran
<code>duperemove</code> at it:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">echo</span> <span class="st">&quot;Creating directory structure, will take a minute&quot;</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> dd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="kw">`</span><span class="fu">seq</span> 1 100<span class="kw">`;</span> <span class="cf">do</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mkdir</span> dd/<span class="va">$d</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f <span class="kw">in</span> <span class="kw">`</span><span class="fu">seq</span> 1 1000<span class="kw">`;</span> <span class="cf">do</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">printf</span> <span class="st">&quot;%*s&quot;</span> 1024 <span class="st">&quot;</span><span class="va">$f</span><span class="st">&quot;</span> <span class="op">&gt;</span> dd/<span class="va">$d</span>/<span class="va">$f</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">done</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">done</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="fu">sync</span></span></code></pre></div>
<p>How long should it take to run? Maybe 1-2 seconds? Alas running it for
real showed the following:</p>
<pre><code>$ time ./duperemove -q -rd dd/
...
Nothing to dedupe.
Total files scanned:  100000
real    0m39,835s
user    1m54,903s
sys     0m8,922s</code></pre>
<p>Almost <code>40 seconds</code> of real time and almost <code>2 minutes</code> of user time
(<code>duperemove</code> runs some actions in parallel) to process <code>100MB</code> of data.</p>
<p>But what did <code>duperemove</code> do all that time? Let’s ask <code>perf</code>:</p>
<pre><code>$ perf record ./duperemove -q -rd dd/
$ perf report

# Overhead  Command       Shared Object            Symbol
# ........  ............  .......................  ...........................................
#
    70.81%  pool          libc.so.6                [.] __memset_avx2_unaligned_erms
     2.14%  duperemove    libsqlite3.so.0.8.6      [.] sqlite3VdbeExec
     0.97%  pool          libsqlite3.so.0.8.6      [.] sqlite3VdbeExec
     0.58%  pool          libc.so.6                [.] __memmove_avx_unaligned_erms
...</code></pre>
<p>Vast majority of the CPU time it spent in <code>memset()</code>!</p>
<h2 id="memset-fix"><code>memset()</code> fix</h2>
<p>There are various ways to find the <code>memset()</code> call. I took the lazy
approach to check where <code>memset()</code> is called with a large value and did
not find any offenders. Then I checked all <code>calloc()</code> calls and found
huge <code>calloc(8MB)</code> allocating temporary space to read files out. This
buffer was allocated at each new opened file.
<a href="https://github.com/markfasheh/duperemove/pull/318">The fix</a> was simple:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode diff"><code class="sourceCode diff"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">--- a/file_scan.c</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="dt">+++ b/file_scan.c</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="dt">@@ -887,7 +887,7 @@ static void csum_whole_file(struct filerec *file,</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        struct block_csum *block_hashes = NULL;</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        memset(&amp;csum_ctxt, 0, sizeof(csum_ctxt));</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="st">-       csum_ctxt.buf = calloc(1, READ_BUF_LEN);</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="va">+       csum_ctxt.buf = malloc(READ_BUF_LEN);</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        assert(csum_ctxt.buf != NULL);</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        csum_ctxt.file = file;</span></code></pre></div>
<p>After the fix profile looked a bit better:</p>
<pre><code>$ time perf record ./duperemove -q -rd dd/
real    0m13,046s
user    0m11,194s
sys     0m2,581s

$ perf report
...
# Overhead  Command       Shared Object            Symbol
# ........  ............  .......................  ..........................................
#
    18.71%  duperemove    libsqlite3.so.0.8.6      [.] sqlite3VdbeExec
     3.18%  pool          libsqlite3.so.0.8.6      [.] sqlite3VdbeExec
     1.95%  pool          libsqlite3.so.0.8.6      [.] sqlite3WhereBegin
     1.66%  pool          libsqlite3.so.0.8.6      [.] resolveExprStep
     1.64%  pool          libsqlite3.so.0.8.6      [.] whereLoopAddBtreeIndex
     1.52%  pool          libc.so.6                [.] __memmove_avx_unaligned_erms
     1.17%  pool          libsqlite3.so.0.8.6      [.] sqlite3_str_vappendf
     1.13%  duperemove    duperemove               [.] populate_tree
     1.10%  pool          libc.so.6                [.] _int_malloc</code></pre>
<p>That was a lot better: almost <code>3x</code> speed up just for removing a single
redundant <code>memset()</code>. It’s a safe change as <code>duperemove</code> guarantees that
it initializes the area with data from the file before calculating the
hash.</p>
<p>Now <code>sqlite</code> is at the top of our profile. Looks like the rest of <code>80%</code>
samples goes to IO wait time.</p>
<h2 id="needless-work-on-small-files">needless work on small files</h2>
<p>While the test seemed to run quickly I noticed that it complains about
dedupe attempts all the time:</p>
<pre><code>$ ./duperemove -q -rd dd/
...
Dedupe for file &quot;dd/20/426&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/427&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/240&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/439&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/377&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/378&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/452&quot; had status (1) &quot;data changed&quot;.
Dedupe for file &quot;dd/20/453&quot; had status (1) &quot;data changed&quot;.</code></pre>
<p>Why does <code>duperemove</code> think data has changed? Those are static files
I just created for test.</p>
<p>The answer was surprising:</p>
<ul>
<li>[good] <code>duperemove</code> skips inline extents in files as file systems can’t
deduplicate data embedded in metadata blocks</li>
<li>[bad] <code>duperemove</code> stores checksum of such files as if they were zero
bytes</li>
<li>[very bad] <code>duperemove</code> tries to deduplicate all these files as they
have identical checksum</li>
</ul>
<p>That was <a href="https://github.com/markfasheh/duperemove/pull/322">easy to fix</a>
as well:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode diff"><code class="sourceCode diff"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">--- a/file_scan.c</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="dt">+++ b/file_scan.c</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="dt">@@ -937,10 +937,19 @@ static void csum_whole_file(struct filerec *file,</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="st">-       ret = dbfile_store_file_digest(db, file, csum_ctxt.file_digest);</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="st">-       if (ret) {</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="st">-               g_mutex_unlock(&amp;io_mutex);</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="st">-               goto err;</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="va">+       /* Do not store files with zero hashable extents. Those are</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="va">+        * usually small files inlined with extent type</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="va">+        * FIEMAP_EXTENT_DATA_INLINE. We avoid storing them as all these</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="va">+        * files have the same zero bytes checksum. Attempt to</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="va">+        * deduplicate those will never succeed and will produce a lot</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="va">+        * of needless work: https://github.com/markfasheh/duperemove/issues/316</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="va">+        */</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="va">+       if (nb_hash &gt; 0) {</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="va">+               ret = dbfile_store_file_digest(db, file, csum_ctxt.file_digest);</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="va">+               if (ret) {</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a><span class="va">+                       g_mutex_unlock(&amp;io_mutex);</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="va">+                       goto err;</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="va">+               }</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        ret = dbfile_commit_trans(db);</span></code></pre></div>
<p>A few lines above <code>duperemove</code> calculates count of non-inline extents
with data in <code>nb_hash</code> variable. If it’s zero then checksum is zero.</p>
<p>The fix as is speeds the scan quite a bit:</p>
<pre><code>$ time ./duperemove -q -rd dd/
...
Simple read and compare of file data found 0 instances of files that might benefit from deduplication.
Nothing to dedupe.
Found 0 identical extents.
Simple read and compare of file data found 0 instances of extents that might benefit from deduplication.
Nothing to dedupe.
Simple read and compare of file data found 0 instances of files that might benefit from deduplication.
Nothing to dedupe.
Found 0 identical extents.
Simple read and compare of file data found 0 instances of extents that might benefit from deduplication.
Nothing to dedupe.
Total files scanned:  100000

real    0m7,844s
user    0m7,116s
sys     0m1,686s</code></pre>
<p>This is an extra <code>2x</code> speed up on tiny files. But we can squeeze a bit
more speed out of it. Notice repetitive <code>Found 0 identical extents</code>
entries. This happens because <code>duperemove</code> batches deduplication
attempts every <code>1024</code> files (controlled by <code>--batchsize=</code> flag). We can
crank up that flag as well:</p>
<pre><code>$ time ./duperemove --batchsize=1000000 -q -rd dd/
Simple read and compare of file data found 0 instances of files that might benefit from deduplication.
Nothing to dedupe.
Found 0 identical extents.
Simple read and compare of file data found 0 instances of extents that might benefit from deduplication.
Nothing to dedupe.
Total files scanned:  100000

real    0m5,995s
user    0m4,605s
sys     0m1,677s</code></pre>
<p>Compared to our initial <code>50s</code> runtime we got <code>~10x</code> speed up.</p>
<p>Question for the reader: what does <code>duperemove</code> do in those 5 seconds?
Is there any room for improvement here?</p>
<p>The above set of fixes sped up <code>duperemove</code> on my real <code>300GB</code> dataset
to finish in 2 minutes!</p>
<p>All done?</p>
<h2 id="dedupe-optionspartial-mode"><code>--dedupe-options=partial</code> mode</h2>
<p>When I skimmed through existing bugs on <code>duperemove</code> bug tracker I
noticed that <code>duperemove</code> only deduplicates extents of identical size
and does not try to look into individual blocks for performance reason.</p>
<p>That sounded a bit strange to me as break up of a file on extents is
quite arbitrary at least on <code>btrfs</code>. You can easily have one extent for
a huge file or a ton of really small ones.</p>
<p>I threw <code>--dedupe-options=partial</code> at <code>300GB</code> and got a CPU-bound hangup
again. An hour later I had to interrupt the process.</p>
<p>This time most of the time was spent in <code>sqlite</code> extent queries for each
individual file. Let’s looks at
<a href="https://github.com/markfasheh/duperemove/pull/324">the fix</a> to get idea
where the problem was hiding:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode diff"><code class="sourceCode diff"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">--- a/dbfile.c</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="dt">+++ b/dbfile.c</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="dt">@@ -1424,9 +1424,8 @@ int dbfile_load_nondupe_file_extents(sqlite3 *db, struct filerec *file,</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        struct file_extent *extents = NULL;</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a> #define NONDUPE_JOIN                                                   \</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="st">-       &quot;FROM extents JOIN (SELECT digest FROM extents GROUP BY digest &quot;\</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="st">-       &quot;HAVING count(*) = 1) AS nondupe_extents on extents.digest = &quot;  \</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="st">-       &quot;nondupe_extents.digest where extents.ino = ?1 and extents.subvol = ?2;&quot;</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="va">+       &quot;FROM extents where extents.ino = ?1 and extents.subvol = ?2 and &quot; \</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="va">+       &quot;(1 = (SELECT COUNT(*) FROM extents as e where e.digest = extents.digest));&quot;</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a> #define GET_NONDUPE_EXTENTS                                            \</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        &quot;select extents.loff, len, poff, flags &quot; NONDUPE_JOIN</span></code></pre></div>
<p>The change switches from this query:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="kw">extents</span>.loff, len, poff, flags</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> <span class="kw">extents</span> <span class="kw">JOIN</span> (</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">SELECT</span> digest</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">FROM</span> <span class="kw">extents</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">GROUP</span> <span class="kw">BY</span> digest</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">HAVING</span> <span class="fu">count</span>(<span class="op">*</span>) <span class="op">=</span> <span class="dv">1</span>) <span class="kw">AS</span> nondupe_extents</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">ON</span> <span class="kw">extents</span>.digest <span class="op">=</span> nondupe_extents.digest</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="kw">WHERE</span> <span class="kw">extents</span>.ino <span class="op">=</span> ?<span class="dv">1</span> <span class="kw">AND</span> <span class="kw">extents</span>.subvol <span class="op">=</span> ?<span class="dv">2</span>;</span></code></pre></div>
<p>To this query:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">SELECT</span> <span class="kw">extents</span>.loff, len, poff, flags</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> <span class="kw">extents</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="kw">WHERE</span> <span class="kw">extents</span>.ino <span class="op">=</span> ?<span class="dv">1</span> <span class="kw">AND</span> <span class="kw">extents</span>.subvol <span class="op">=</span> ?<span class="dv">2</span> <span class="kw">AND</span> (</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span> <span class="op">=</span> (<span class="kw">SELECT</span> <span class="fu">COUNT</span>(<span class="op">*</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>         <span class="kw">FROM</span> <span class="kw">extents</span> <span class="kw">as</span> e</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>         <span class="kw">WHERE</span> e.digest <span class="op">=</span> <span class="kw">extents</span>.digest));</span></code></pre></div>
<p>Both queries should produce identical result and both do two things:</p>
<ol type="1">
<li>Fetch all entries from <code>extents</code> table for a given <code>inode</code> (<code>ino</code>
and <code>subvol</code>)</li>
<li>Pick only those extents that have unique digest: so that extents are
not shared and thus splitting them into smaller extents should not be
a problem.</li>
</ol>
<p>Here are both plans for both queries told by <code>sqlite</code>. Enabling plan
dump:</p>
<pre><code>$ sqlite3 /tmp/foo.db
sqlite&gt; .eqp on</code></pre>
<p>First plan:</p>
<pre><code>sqlite&gt; SELECT extents.loff, len, poff, flags
       FROM extents JOIN (
         SELECT digest
         FROM extents
         GROUP BY digest
         HAVING count(*) = 1) AS nondupe_extents
       on extents.digest =  nondupe_extents.digest
       where extents.ino = ?1 and extents.subvol = ?2;

QUERY PLAN
|--CO-ROUTINE nondupe_extents
|  `--SCAN extents USING COVERING INDEX idx_extent_digest
|--SEARCH extents USING INDEX idx_extents_inosub (ino=? AND subvol=?)
|--BLOOM FILTER ON nondupe_extents (digest=?)
`--SEARCH nondupe_extents USING AUTOMATIC COVERING INDEX (digest=?)</code></pre>
<p>Second plan:</p>
<pre><code>sqlite&gt; SELECT extents.loff, len, poff, flags
        FROM extents
        where extents.ino = ?1 and extents.subvol = ?2 and (
          1 = (SELECT COUNT(*)
              FROM extents as e
              where e.digest = extents.digest));

QUERY PLAN
|--SEARCH extents USING INDEX idx_extents_inosub (ino=? AND subvol=?)
`--CORRELATED SCALAR SUBQUERY 1
   `--SEARCH e USING COVERING INDEX idx_extent_digest (digest=?)</code></pre>
<p>The first plan is more complicated. One of it’s problems is the use of
<code>SCAN</code> (full table scan) in <code>CO-ROUTINE nondupe_extents</code>. As I understand
the output here full <code>extents</code> table scan is performed at least once for
this whole query.</p>
<p>Reading the second plan is easy: all searches use existing indexes in
the tables. We fetch all extents for the <code>inode</code> and then leave only
those that match a subquery. Subquery also uses only index lookup.</p>
<p>Now the whole non-incremental <code>duperemove</code> run on my <code>300GB</code> dataset
takes 9 minutes. And in incremental mode it takes about 4 minutes.</p>
<p>Yay!</p>
<h2 id="parting-words">Parting words</h2>
<p>It took me a few steps to get <code>duperemove</code> to work on my machines:</p>
<ul>
<li><a href="https://github.com/markfasheh/duperemove/pull/269">Increase file descriptor limit</a></li>
<li><a href="https://github.com/markfasheh/duperemove/pull/318">Avoid redundant <code>memset(8MB)</code> on file read</a></li>
<li><a href="https://github.com/markfasheh/duperemove/pull/322">Do not deduplicate inline-only files</a></li>
<li><a href="https://github.com/markfasheh/duperemove/pull/324">Avoid full table scan in <code>partial</code> mode</a></li>
</ul>
<p>This sped up <code>duperemove</code> run from multiple hours down to under 10
minutes on a few hundreds of gigabytes of small files.</p>
<p><code>duperemove</code> still has quite a bit room for improvement to get even more
performance.</p>
<p>Fun fact: <code>duperemove</code> uses two simple <code>ioctl()</code> interfaces:</p>
<ul>
<li><code>FS_IOC_FIEMAP</code> to get on-disk layout for a file:
<a href="https://docs.kernel.org/filesystems/fiemap.html" class="uri">https://docs.kernel.org/filesystems/fiemap.html</a>.</li>
<li><code>FIDEDUPERANGE</code> to deduplicate file range between two file
descriptors.</li>
</ul>
<p>Have fun!</p>
        </div>
    </body>
</html>
