<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>trofi's blog: mitigating bufferbloat</title>
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../css/code.css" />
        <link rel="stylesheet" type="text/css" href="../css/highlighting-kate.css" />

        <link rel="alternate" type="application/atom+xml" href="../feed/atom.xml" title="Atom 1.0" />
        <link rel="alternate" type="application/rss+xml" href="../feed/rss.xml" title="RSS 2.0" />
        <link rel="shortcut icon" href="../images/favicon.ico" />
    </head>
    <body>
        <div id="navigation">
            <a href="../">/</a>
            <a href="../archive.html">/archive</a>
            <a href="../feed/atom.xml">/atom</a>
            <a href="../feed/rss.xml">/rss</a>
        </div>

        <div id="content">
            <h1>mitigating bufferbloat</h1>

            <h1 id="tldr">Tl;DR</h1>
<p>When I upload something large from my laptop my network latency increases
from <strong>10 milliseconds</strong> up to <strong>2-4 seconds</strong>. That makes remote ssh, audio,
video and even browsing quite a challenging task. I avoid latency increase
and network congestion by aggressively throttling all my device with this
simple command from
<a href="https://www.bufferbloat.net/projects/codel/wiki/Cake/">CAKE howto</a>:</p>
<pre class><code># tc qdisc del dev wl0 root
# tc qdisc add dev wl0 root cake bandwidth 1mbit conservative</code></pre>
<p>Done! Now I almost never notice latency increase even if I upload
A Lot of stuff.</p>
<h1 id="more-words">More words</h1>
<p>If you are experiencing severe networking lags (<strong>10x</strong> latency increase)
when network is congested then you probably experience a <a href="https://www.bufferbloat.net/projects/">bufferbloat effect</a>
where networking devices keep your packets in their large software/hardware queues.</p>
<p>These queues can hide anywhere: in your own networking card(!), in your
wi-fi router, somewhere in the middle of The Internet.
It is a bug in the implementation of those devices.</p>
<p>Excessive buffering usually happens on a last mile of your device
connected to <strong>ISP</strong>. <strong>ISP</strong>s usually have a lot more bandwidth and
smarter devices to avoid that effect.</p>
<h1 id="observing-bufferbloat">Observing bufferbloat</h1>
<p>Before doing anything about it let’s check if we have a problem:</p>
<pre class><code>$ ping 8.8.8.8
64 bytes from 8.8.8.8: icmp_seq=1 ttl=54 time=8.53 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=54 time=18.1 ms
64 bytes from 8.8.8.8: icmp_seq=3 ttl=54 time=9.47 ms
64 bytes from 8.8.8.8: icmp_seq=4 ttl=54 time=14.1 ms
...
# in a second terminal start uploader
$ scp /boot/vmlinuz slyfox@guppy.ia64.dev.gentoo.org:/tmp/my-kernel
vmlinuz 99% 7968KB 119.1KB/s   00:00 ETA
...
# in the first terminal:
64 bytes from 8.8.8.8: icmp_seq=100 ttl=54 time=116 ms
64 bytes from 8.8.8.8: icmp_seq=101 ttl=54 time=141 ms
64 bytes from 8.8.8.8: icmp_seq=102 ttl=54 time=192 ms
64 bytes from 8.8.8.8: icmp_seq=103 ttl=54 time=188 ms
64 bytes from 8.8.8.8: icmp_seq=104 ttl=54 time=182 ms
64 bytes from 8.8.8.8: icmp_seq=105 ttl=54 time=218 ms
64 bytes from 8.8.8.8: icmp_seq=166 ttl=54 time=223 ms
64 bytes from 8.8.8.8: icmp_seq=167 ttl=54 time=230 ms
64 bytes from 8.8.8.8: icmp_seq=168 ttl=54 time=232 ms
64 bytes from 8.8.8.8: icmp_seq=169 ttl=54 time=8.66 ms &lt;--- recovered
64 bytes from 8.8.8.8: icmp_seq=170 ttl=54 time=9.21 ms
64 bytes from 8.8.8.8: icmp_seq=171 ttl=54 time=8.79 ms
64 bytes from 8.8.8.8: icmp_seq=172 ttl=54 time=9.02 ms
64 bytes from 8.8.8.8: icmp_seq=173 ttl=54 time=10.2 ms</code></pre>
<p>You can notice where upload started and finished just by looking
at the ping latency output.</p>
<p>There is a nice interactive website where you can profile
your network in a slightly more rigorous way: <a href="https://www.dslreports.com/speedtest">https://www.dslreports.com/speedtest</a></p>
<p>My profile on default system: <a href="https://www.dslreports.com/speedtest/62838367">https://www.dslreports.com/speedtest/62838367</a></p>
<p>We can see a bunch of things outright:</p>
<ul>
<li>16.5 mbit/s download and 0.96 mbit/s upload speed</li>
<li>26ms of idle latency (to closest test server perhaps)</li>
<li>latency increases a bit when I congest network with download: 26ms-&gt;70ms (100ms worst)</li>
<li>latency increases A Lot when I congest network with upload: 26ms-&gt;~1s (~2.6s worst)</li>
</ul>
<p>These were my defaults during first test:</p>
<pre class><code>$ tc qdisc show dev wl0
qdisc mq 0: root
qdisc fq_codel 0: parent :4 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
qdisc fq_codel 0: parent :3 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
qdisc fq_codel 0: parent :2 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn</code></pre>
<p>Now let’s artificially limit egress (outgoing) bandwidth:</p>
<pre class><code># tc qdisc del dev wl0 root
# tc qdisc add dev wl0 root cake bandwidth 1mbit conservative</code></pre>
<p>and retest again: <a href="https://www.dslreports.com/speedtest/62838672">https://www.dslreports.com/speedtest/62838672</a></p>
<p>Things changed:</p>
<ul>
<li>Speeds decreased slightly: 16.1 mbit/s upload, 0.811 mbit/s download.
Mostly because I set <strong>conservative</strong> keyword to account for worst encapsulation overhead.
You can experiment with more fine-grained values, but I don’t mind the result.</li>
<li>Download congestion can still impact latency increase. For some reason
it’s more pronounced now: 140ms worst.</li>
<li>Upload congestion has almost no effect on latency! average is 34ms, worst is 70ms!</li>
</ul>
<p>Now let’s also try artificially limiting ingress:</p>
<pre class><code>#### egress as before
# tc qdisc del dev wl0 root
# tc qdisc add dev wl0 root cake bandwidth 1mbit conservative

#### ingress, a bit kludgy
# ip link add name ifb4wl0 type ifb
# tc qdisc del dev wl0 ingress
# tc qdisc add dev wl0 handle ffff: ingress
# tc qdisc del dev ifb4wl0 root
# tc qdisc add dev ifb4wl0 root cake bandwidth 16mbit conservative
# ip link set ifb4wl0 up
# tc filter add dev wl0 parent ffff: protocol all prio 10 u32 match u32 0 0 flowid 1:1 action mirred egress redirect dev ifb4wl0</code></pre>
<p>and retest again: <a href="https://www.dslreports.com/speedtest/62838933">https://www.dslreports.com/speedtest/62838933</a></p>
<ul>
<li>Speeds decreased even more: 11.27 mbit/s upload, 0.766 mbit/s download.
I think the drastic drops upset TCP and it can’t recover faster.</li>
<li>Download congestion did not become any better. That might
be a sign of bufferbloat on ISP side.</li>
<li>Upload congestion is still good.</li>
</ul>
<p>If you rerun the test you will notice that numbers will change from run to run
in order of 10-20% (or more). This will not allow you to fine-tune the setup
but is good enough to observe the effect.</p>
<h1 id="more-fine-grained-testing">More fine-grained testing</h1>
<p>To get better numbers bufferbloat.net project suggests using <a href="https://flent.org/intro.html#quick-start">flent</a>
command line tool.</p>
<p>For that you will need a peer host to talk to. I’m picking guppy.ia64.dev.gentoo.org
140ms (half a globe) away from me.</p>
<p>Here is the run on default <strong>qdisc</strong>:</p>
<pre class><code>#### remotely
guppy / # netserver -D

#### locally
localhost # tc qdisc show dev wl0
qdisc mq 0: root
qdisc fq_codel 0: parent :4 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
qdisc fq_codel 0: parent :3 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
qdisc fq_codel 0: parent :2 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn
qdisc fq_codel 0: parent :1 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms memory_limit 32Mb ecn

localhost # flent rrul -p all_scaled -l 60 -H guppy.ia64.dev.gentoo.org -t def-to-guppy -o def-filename.png
Started Flent 1.0.1 using Python 3.6.10.
Starting rrul test. Expected run time: 70 seconds.</code></pre>
<p><a href="../posts.data/217-bufferbloat/def-filename.png"><img src="../posts.data/217-bufferbloat/def-filename.png" alt="image" /></a></p>
<p>There are 3 graphs:</p>
<ul>
<li>download speed: jumps around 2.5-5 mbit/s.</li>
<li>uplaod speed: very unstable, hard to say anything :)</li>
<li>latency graph: almost always 750ms, sometimes touches 500ms</li>
</ul>
<p>Here is the run with <strong>cake</strong> both on ingress and egress:</p>
<pre class><code>#### remotely
guppy / # netserver -D

#### locally
localhost # tc qdisc show dev wl0
qdisc cake 800f: root refcnt 5 bandwidth 1Mbit diffserv3 triple-isolate nonat nowash no-ack-filter split-gso rtt 100.0ms atm overhead 48
qdisc ingress ffff: parent ffff:fff1 ----------------

localhost # flent rrul -p all_scaled -l 60 -H guppy.ia64.dev.gentoo.org -t cake-to-guppy -o cake-filename.png
Started Flent 1.0.1 using Python 3.6.10.
Starting rrul test. Expected run time: 70 seconds.</code></pre>
<p><a href="../posts.data/217-bufferbloat/cake-filename.png"><img src="../posts.data/217-bufferbloat/cake-filename.png" alt="image" /></a></p>
<p>3 graphs again:</p>
<ul>
<li>download speed: jumps around 2-3 mbit/s.</li>
<li>uplaod speed: looks like 0.1mbit/s?</li>
<li>latency graph: most of the time 170ms, sometimes spikes up to 300ms</li>
</ul>
<p>Even though data is still quite noisy we can see that latency is lower
in the second case.</p>
<p>Picking closer host and running test for longer time might yield more
precise data.</p>
<p>Now you can tweak many more parameters to see what changes for better
or worse: TCP congestion algorithms (I use <strong>bbr</strong>), many tweaks
on <strong>man tc-cake</strong>’s side and many more.</p>
<p>I suggest skimming through more resources at <a href="https://www.bufferbloat.net/projects/">https://www.bufferbloat.net/projects/</a>
if you are interested in the topic.</p>
<h1 id="final-test">Final test</h1>
<p>Let’s get back to our original test (with ingress/egress <strong>cake</strong> enabled this time):</p>
<pre class><code>$ ping 8.8.8.8
64 bytes from 8.8.8.8: icmp_seq=1 ttl=54 time=9.31 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=54 time=9.20 ms
64 bytes from 8.8.8.8: icmp_seq=3 ttl=54 time=8.37 ms
64 bytes from 8.8.8.8: icmp_seq=4 ttl=54 time=10.6 ms
...
# in a second terminal start uploader
$ scp /boot/vmlinuz slyfox@guppy.ia64.dev.gentoo.org:/tmp/my-kernel
vmlinuz 99% 7968KB 104.6KB/s   00:00 ETA
...
# in the first terminal:
64 bytes from 8.8.8.8: icmp_seq=178 ttl=54 time=16.5 ms
64 bytes from 8.8.8.8: icmp_seq=179 ttl=54 time=14.8 ms
64 bytes from 8.8.8.8: icmp_seq=180 ttl=54 time=15.4 ms
64 bytes from 8.8.8.8: icmp_seq=181 ttl=54 time=10.0 ms
64 bytes from 8.8.8.8: icmp_seq=182 ttl=54 time=20.3 ms
64 bytes from 8.8.8.8: icmp_seq=183 ttl=54 time=9.16 ms
64 bytes from 8.8.8.8: icmp_seq=184 ttl=54 time=9.71 ms
64 bytes from 8.8.8.8: icmp_seq=185 ttl=54 time=17.5 ms
64 bytes from 8.8.8.8: icmp_seq=186 ttl=54 time=13.0 ms
64 bytes from 8.8.8.8: icmp_seq=224 ttl=54 time=19.9 ms
64 bytes from 8.8.8.8: icmp_seq=225 ttl=54 time=42.3 ms
64 bytes from 8.8.8.8: icmp_seq=226 ttl=54 time=13.3 ms
64 bytes from 8.8.8.8: icmp_seq=227 ttl=54 time=8.39 ms
64 bytes from 8.8.8.8: icmp_seq=230 ttl=54 time=11.4 ms
64 bytes from 8.8.8.8: icmp_seq=231 ttl=54 time=8.84 ms &lt;--- recovered
64 bytes from 8.8.8.8: icmp_seq=232 ttl=54 time=8.63 ms
64 bytes from 8.8.8.8: icmp_seq=233 ttl=54 time=8.49 ms</code></pre>
<p>Latency increased a bit from 10ms to ~20ms. But it does not get to 300ms. Yay!</p>
<p>Have fun!</p>

<div class="info">
    Posted on April 26, 2020 by trofi. <a href="mailto:slyich@gmail.com">Email</a>,
    <a href="https://github.com/trofi/trofi.github.io.gen">pull requests or comments</a>
    are welcome!
</div>

        </div>
        <div id="footer">
            powered by <a href="http://jaspervdj.be/hakyll">hakyll</a>
        </div>
    </body>
</html>
